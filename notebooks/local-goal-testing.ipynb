{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.444171Z",
     "start_time": "2025-04-08T12:31:33.440575Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.document_converter import (\n",
    "    DocumentConverter,\n",
    "    PdfFormatOption,\n",
    "    WordFormatOption,\n",
    "    HTMLFormatOption\n",
    ")\n",
    "from docling.backend.msword_backend import MsWordDocumentBackend\n",
    "from docling.backend.html_backend import HTMLDocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.converters.pdfminer import PDFMinerToDocument\n",
    "from haystack.components.converters.docx import DOCXToDocument # require python-docx\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file.html import HTMLTagReader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "import editdistance\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from jiwer import wer\n",
    "\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    EasyOcrOptions,\n",
    "    # OcrMacOptions,\n",
    "    PdfPipelineOptions,\n",
    "    RapidOcrOptions,\n",
    "    TesseractOcrOptions,\n",
    ")\n",
    "from langchain_community.document_loaders.parsers import TesseractBlobParser\n",
    "from langchain_community.document_loaders.parsers import RapidOCRBlobParser\n",
    "\n",
    "from llmsherpa.readers import LayoutPDFReader\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.449465Z",
     "start_time": "2025-04-08T12:31:33.447140Z"
    }
   },
   "source": [
    "load_dotenv(dotenv_path=\"../llama-api-key.env\")\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.498107Z",
     "start_time": "2025-04-08T12:31:33.491738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DOCUMENT LOADERS INITIALISATION\n",
    "# avoid re-initialisation for each function call\n",
    "\n",
    "## DOCLING\n",
    "default_converter_docling = DocumentConverter()\n",
    "\n",
    "PYPDFium_converter_docling = DocumentConverter(\n",
    "    format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                backend=PyPdfiumDocumentBackend\n",
    "            )\n",
    "        }\n",
    ")\n",
    "\n",
    "DOCX_converter_docling = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.DOCX: WordFormatOption(\n",
    "            backend=MsWordDocumentBackend  # default backend for DOCX files\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "HTML_converter_docling = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.HTML: HTMLFormatOption(\n",
    "            backend=HTMLDocumentBackend  # default backend for HTML files\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "## HAYSTACK\n",
    "PYPDF_converter_haystack = PyPDFToDocument()\n",
    "\n",
    "PDFMiner_converter_haystack = PDFMinerToDocument()\n",
    "\n",
    "DOCX_converter_haystack=DOCXToDocument()\n",
    "\n",
    "HTML_converter_haystack=HTMLToDocument()\n",
    "\n",
    "## LLAMAINDEX\n",
    "llamaparse_llamaindex = LlamaParse(\n",
    "   result_type=\"text\"\n",
    ")\n",
    "\n",
    "HTML_converter_llamaindex=HTMLTagReader(tag='html')\n",
    "\n",
    "\n",
    "## LANGCHAIN\n",
    "# DONE IN FUNCTION DIRECTLY\n",
    "\n",
    "\n",
    "converter_llmsherpa =LayoutPDFReader(\"http://localhost:5010/api/parseDocument?renderFormat=all\")\n",
    "#if you want to test (deply the following container docker):\n",
    "# $ sudo docker pull ghcr.io/nlmatics/nlm-ingestor:latest\n",
    "# $ sudo docker run -p 5010:5001 ghcr.io/nlmatics/nlm-ingestor:latest"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.660872Z",
     "start_time": "2025-04-08T12:31:33.655758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DOCUMENT LOADER FUNCTIONS\n",
    "\n",
    "def docling_default(file, converter=default_converter_docling):\n",
    "    result=converter.convert(file)\n",
    "    return result.document.export_to_text()\n",
    "\n",
    "def docling_pyPDFium(file, converter=PYPDFium_converter_docling):\n",
    "    result=converter.convert(file)\n",
    "    return result.document.export_to_text()\n",
    "\n",
    "def docling_DOCX(file, converter=DOCX_converter_docling):\n",
    "    result=converter.convert(file)\n",
    "    return result.document.export_to_text()\n",
    "\n",
    "def docling_HTML(file, converter=HTML_converter_docling):\n",
    "    result=converter.convert(file)\n",
    "    return result.document.export_to_text()\n",
    "\n",
    "def haystack_pyPDF(file, converter=PYPDF_converter_haystack):\n",
    "    result=converter.run(sources=[file])\n",
    "    documents_text='\\n'.join([r.content for r in result['documents']])\n",
    "    return documents_text\n",
    "\n",
    "def haystack_PDFMiner(file, converter=PDFMiner_converter_haystack):\n",
    "    result=converter.run(sources=[file])\n",
    "    documents_text='\\n'.join([r.content for r in result['documents']])\n",
    "    return documents_text\n",
    "\n",
    "def haystack_DOCX(file, converter=DOCX_converter_haystack):\n",
    "    result=converter.run(sources=[file])\n",
    "    documents_text='\\n'.join([r.content for r in result['documents']])\n",
    "    return documents_text\n",
    "\n",
    "def haystack_HTML(file, converter=HTML_converter_haystack):\n",
    "    result=converter.run(sources=[file])\n",
    "    documents_text='\\n'.join([r.content for r in result['documents']])\n",
    "    return documents_text\n",
    "\n",
    "def llamaindex_llamaparse(file, converter=llamaparse_llamaindex):\n",
    "    result=converter.load_data(file)\n",
    "    documents_text='\\n'.join([r.text for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def llamaindex_simpleDirectoryReader(file):\n",
    "    converter=SimpleDirectoryReader(input_files=[file])\n",
    "    result=converter.load_data()\n",
    "    documents_text='\\n'.join([r.text for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def llamaindex_HTML(file, converter=HTML_converter_llamaindex):\n",
    "    result=converter.load_data(file)\n",
    "    documents_text='\\n'.join([r.text for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_pyPDF(file):\n",
    "    result=PyPDFLoader(file).load()\n",
    "    documents_text='\\n'.join([r.page_content for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_PDFPlumber(file):\n",
    "    result=PDFPlumberLoader(file).load()\n",
    "    documents_text='\\n'.join([r.page_content for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_PyPDFium2(file):\n",
    "    result=PyPDFium2Loader(file).load()\n",
    "    documents_text='\\n'.join([r.page_content for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_PyMUPDF(file):\n",
    "    result=PyMuPDFLoader(file).load()\n",
    "    documents_text='\\n'.join([r.page_content for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_PDFMiner(file):\n",
    "    result=PDFMinerLoader(file).load()\n",
    "    documents_text='\\n'.join([r.page_content for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_Docx2txt(file):\n",
    "    result=Docx2txtLoader(file).load()\n",
    "    documents_text='\\n'.join([r.page_content for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_UnstructuredHTML(file):\n",
    "    result=UnstructuredHTMLLoader(file).load()\n",
    "    documents_text='\\n'.join([r.page_content for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_BSHtml(file):\n",
    "    result=BSHTMLLoader(file).load()\n",
    "    documents_text='\\n'.join([r.page_content for r in result])\n",
    "    return documents_text\n",
    "\n",
    "def llmsherpa_default(path,converter = converter_llmsherpa):\n",
    "    llmsherpa_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all\"\n",
    "    pdf_url = os.path.abspath(path)# also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "    pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "    doc = pdf_reader.read_pdf(pdf_url)\n",
    "    result=''\n",
    "    for chunk in doc.chunks():\n",
    "        result += chunk.to_text()\n",
    "        result += '\\n'\n",
    "    return result\n",
    "\n",
    "dl_func_mapping={\n",
    "    'docling_default':docling_default,\n",
    "    'docling_pyPDFium':docling_pyPDFium,\n",
    "    'docling_DOCX':docling_DOCX,\n",
    "    'docling_HTML':docling_HTML,\n",
    "    'haystack_pyPDF':haystack_pyPDF,\n",
    "    'haystack_PDFMiner':haystack_PDFMiner,\n",
    "    'haystack_docx': haystack_DOCX,\n",
    "    'haystack_html': haystack_HTML,\n",
    "    'llamaindex_llamaparse':llamaindex_llamaparse,\n",
    "    'llamaindex_simpleDirectoryReader':llamaindex_simpleDirectoryReader,\n",
    "    'llamaindex_HTMLTagReader':llamaindex_HTML,\n",
    "    'langchain_pyPDF':langchain_pyPDF,\n",
    "    'langchain_PDFPlumber':langchain_PDFPlumber,\n",
    "    'langchain_PyPDFium2':langchain_PyPDFium2,\n",
    "    'langchain_PyMUPDF':langchain_PyMUPDF,\n",
    "    'langchain_PDFMiner':langchain_PDFMiner,\n",
    "    'langchain_Docx2txt':langchain_Docx2txt,\n",
    "    'langchain_UnstructuredHTML':langchain_UnstructuredHTML,\n",
    "    'langchain_BSHtml':langchain_BSHtml,\n",
    "    'llmsherpa_default':llmsherpa_default,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.709834Z",
     "start_time": "2025-04-08T12:31:33.703652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DOCUMENT LOADER FUNCTIONS\n",
    "\n",
    "def docling_default_metadata(file, converter=default_converter_docling):\n",
    "    result=converter.convert(file)\n",
    "    result_string=\"\"\n",
    "    for key,value in result.document.model_dump().items():\n",
    "        result_string+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return result_string\n",
    "\n",
    "def docling_pyPDFium_metadata(file, converter=PYPDFium_converter_docling):\n",
    "    result=converter.convert(file)\n",
    "    result_string=\"\"\n",
    "    for key,value in result.document.model_dump().items():\n",
    "        result_string+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return result_string\n",
    "\n",
    "def docling_DOCX_metadata(file, converter=DOCX_converter_docling):\n",
    "    result=converter.convert(file)\n",
    "    result_string=\"\"\n",
    "    for key,value in result.document.model_dump().items():\n",
    "        result_string+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return result_string\n",
    "\n",
    "def docling_HTML_metadata(file, converter=HTML_converter_docling):\n",
    "    result=converter.convert(file)\n",
    "    result_string=\"\"\n",
    "    for key,value in result.document.model_dump().items():\n",
    "        result_string+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return result_string\n",
    "\n",
    "def haystack_pyPDF_metadata(file, converter=PYPDF_converter_haystack):\n",
    "    result=converter.run(sources=[file])\n",
    "    documents_meta=''\n",
    "    for r in result['documents']:\n",
    "        for key,value in r.meta.items():\n",
    "            documents_meta+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_meta\n",
    "\n",
    "def haystack_PDFMiner_metadata(file, converter=PDFMiner_converter_haystack):\n",
    "    result=converter.run(sources=[file])\n",
    "    documents_meta=''\n",
    "    for r in result['documents']:\n",
    "        for key,value in r.meta.items():\n",
    "            documents_meta+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_meta\n",
    "\n",
    "def haystack_DOCX_metadata(file, converter=DOCX_converter_haystack):\n",
    "    result=converter.run(sources=[file])\n",
    "    documents_meta=''\n",
    "    for r in result['documents']:\n",
    "        for key,value in r.meta.items():\n",
    "            documents_meta+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_meta\n",
    "\n",
    "def haystack_HTML_metadata(file, converter=HTML_converter_haystack):\n",
    "    result=converter.run(sources=[file])\n",
    "    documents_meta=''\n",
    "    for r in result['documents']:\n",
    "        for key,value in r.meta.items():\n",
    "            documents_meta+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_meta\n",
    "\n",
    "def llamaindex_llamaparse_metadata(file, converter=llamaparse_llamaindex):\n",
    "    result=converter.load_data(file)\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def llamaindex_simpleDirectoryReader_metadata(file):\n",
    "    converter=SimpleDirectoryReader(input_files=[file])\n",
    "    result=converter.load_data()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def llamaindex_HTML_metadata(file, converter=HTML_converter_llamaindex):\n",
    "    result=converter.load_data(file)\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def langchain_pyPDF_metadata(file):\n",
    "    result=PyPDFLoader(file).load()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def langchain_PDFPlumber_metadata(file):\n",
    "    result=PDFPlumberLoader(file).load()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def langchain_PyPDFium2_metadata(file):\n",
    "    result=PyPDFium2Loader(file).load()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def langchain_PyMUPDF_metadata(file):\n",
    "    result=PyMuPDFLoader(file).load()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def langchain_PDFMiner_metadata(file):\n",
    "    result=PDFMinerLoader(file).load()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def langchain_Docx2txt_metadata(file):\n",
    "    result=Docx2txtLoader(file).load()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def langchain_UnstructuredHTML_metadata(file):\n",
    "    result=UnstructuredHTMLLoader(file).load()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def langchain_BSHtml_metadata(file):\n",
    "    result=BSHTMLLoader(file).load()\n",
    "    documents_metadata=\"\"\n",
    "    for r in result:\n",
    "        for key,value in r.metadata.items():\n",
    "            documents_metadata+=key+\" : \"+str(value)+\"\\n\"\n",
    "    return documents_metadata\n",
    "\n",
    "def llmsherpa_default_metadata(path,converter = converter_llmsherpa):\n",
    "    # LLMSHERPA DOES NOT PARSE METADATA\n",
    "    return \"\"\n",
    "\n",
    "dl_func_mapping_metadata={\n",
    "    'docling_default':docling_default_metadata,\n",
    "    'docling_pyPDFium':docling_pyPDFium_metadata,\n",
    "    'docling_DOCX':docling_DOCX_metadata,\n",
    "    'docling_HTML':docling_HTML_metadata,\n",
    "    'haystack_pyPDF':haystack_pyPDF_metadata,\n",
    "    'haystack_PDFMiner':haystack_PDFMiner_metadata,\n",
    "    'haystack_docx': haystack_DOCX_metadata,\n",
    "    'haystack_html': haystack_HTML_metadata,\n",
    "    'llamaindex_llamaparse':llamaindex_llamaparse_metadata,\n",
    "    'llamaindex_simpleDirectoryReader':llamaindex_simpleDirectoryReader_metadata,\n",
    "    'llamaindex_HTMLTagReader':llamaindex_HTML_metadata,\n",
    "    'langchain_pyPDF':langchain_pyPDF_metadata,\n",
    "    'langchain_PDFPlumber':langchain_PDFPlumber_metadata,\n",
    "    'langchain_PyPDFium2':langchain_PyPDFium2_metadata,\n",
    "    'langchain_PyMUPDF':langchain_PyMUPDF_metadata,\n",
    "    'langchain_PDFMiner':langchain_PDFMiner_metadata,\n",
    "    'langchain_Docx2txt':langchain_Docx2txt_metadata,\n",
    "    'langchain_UnstructuredHTML':langchain_UnstructuredHTML_metadata,\n",
    "    'langchain_BSHtml':langchain_BSHtml_metadata,\n",
    "    'llmsherpa_default':llmsherpa_default_metadata,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.810969Z",
     "start_time": "2025-04-08T12:31:33.798297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# OCR PARSERS\n",
    "\n",
    "def langchain_pyPDF_tesseract(file):\n",
    "    langchain_tesseract_converter = PyPDFLoader(\n",
    "        file,\n",
    "        mode=\"page\",\n",
    "        images_inner_format=\"html-img\",\n",
    "        images_parser=TesseractBlobParser(),\n",
    "    )\n",
    "    docs=langchain_tesseract_converter.load()\n",
    "    documents_text='\\n'.join([r.page_content for r in docs])\n",
    "    return documents_text\n",
    "\n",
    "def langchain_PyMuPDFLoader_tesseract(file):\n",
    "    langchain_tesseract_converter = PyMuPDFLoader(\n",
    "        file,\n",
    "        mode=\"page\",\n",
    "        images_inner_format=\"html-img\",\n",
    "        images_parser=TesseractBlobParser(),\n",
    "    )\n",
    "    docs=langchain_tesseract_converter.load()\n",
    "    documents_text='\\n'.join([r.page_content for r in docs])\n",
    "    return documents_text\n",
    "\n",
    "\n",
    "# docling\n",
    "ocr_options_tesseract = TesseractOcrOptions(force_full_page_ocr=True)\n",
    "tesseract_pipeline_options = PdfPipelineOptions(\n",
    "    do_ocr=True\n",
    ")\n",
    "\n",
    "tesseract_pipeline_options.ocr_options = ocr_options_tesseract\n",
    "docling_tesseract_converter=DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=tesseract_pipeline_options\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "def docling_tesseract(file,converter=docling_tesseract_converter):\n",
    "    result=converter.convert(file)\n",
    "    return result.document.export_to_text()\n",
    "\n",
    "#CANT FORCE OCR ON DOCLING HTML!!!!\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.847133Z",
     "start_time": "2025-04-08T12:31:33.845665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ocr_dl_func_mapping={\n",
    "    'langchain_pyPDF_tesseract':langchain_pyPDF_tesseract,\n",
    "    'langchain_PyMuPDFLoader_tesseract':langchain_PyMuPDFLoader_tesseract,\n",
    "    'docling_tesseract': docling_tesseract,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.897311Z",
     "start_time": "2025-04-08T12:31:33.889543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Edge cases: OCR POISONOING, FONT POISONING\n",
    "# OCR -> take as reference original text, and compare to text extracted\n",
    "\n",
    "#making working in different machines\n",
    "cwd = os.getcwd()\n",
    "pwd = os.path.dirname(cwd)\n",
    "ocr_mapping_df = pd.read_csv(os.path.join(pwd,'ocr_mapping.csv'))\n",
    "\n",
    "# Character Error Rate (CER)\n",
    "def cer(reference, hypothesis):\n",
    "    return editdistance.eval(reference, hypothesis) / max(len(reference), 1)\n",
    "\n",
    "def normalized_levenshtein(reference, hypothesis):\n",
    "    distance = editdistance.eval(reference, hypothesis)\n",
    "    max_len = max(len(reference), len(hypothesis), 1)\n",
    "    return distance / max_len\n",
    "\n",
    "def test_ocr(df, dl, dl_func, ocr_folder_path, input_format, metrics):\n",
    "    dir_name=os.path.basename(ocr_folder_path)\n",
    "    # Get attack name, technique, and files\n",
    "    attack, technique= dir_name.split('_')\n",
    "    files=[os.path.join(ocr_folder_path,f) for f in os.listdir(ocr_folder_path)]\n",
    "    # print(f'    attack: {attack}, technique: {technique}')\n",
    "    records_to_add=[]\n",
    "    for file in tqdm(files,desc=f'parsing files with {dl}'):\n",
    "        \n",
    "        # get original text from mapping csv\n",
    "        filename=os.path.basename(file)\n",
    "\n",
    "        # Extract text\n",
    "        extracted_text=dl_func(file)\n",
    "        #if filename not in ocr_mapping_df, throw exception\n",
    "        if file not in ocr_mapping_df['full_file_path'].values:\n",
    "            test_result=\"NULL, FILE MAPPING WITH ORIGINAL TEXT FAILED\"\n",
    "        else:\n",
    "            original_text=ocr_mapping_df[ocr_mapping_df['full_file_path']==file]['joint_text'].values[0]\n",
    "            if metrics=='CER':\n",
    "                test_result=cer(original_text,extracted_text)\n",
    "            elif metrics=='levenshtein':\n",
    "                test_result=normalized_levenshtein(original_text,extracted_text)\n",
    "            elif metrics=='WER':\n",
    "                test_result=wer(original_text,extracted_text)\n",
    "            elif metrics=='BLEU':\n",
    "                test_result=sentence_bleu([original_text.split()],extracted_text.split())\n",
    "            else:\n",
    "                raise Exception('Invalid metrics specified! Available metrics: CER, levenshtein, WER, BLEU')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # Add new record to dataframe\n",
    "        records_to_add.append({\n",
    "            'FileFormat': input_format,\n",
    "            'Filename': filename,\n",
    "            'AttackFamily': 'Data Obfuscation',\n",
    "            'AttackName': attack,\n",
    "            'Technique': technique,\n",
    "            'DocumentLoader': dl,\n",
    "            'AttackResult': test_result,\n",
    "            'Text_extracted': extracted_text\n",
    "        })\n",
    "    # Add records to dataframe\n",
    "    df=pd.concat([df,pd.DataFrame.from_records(records_to_add)])  \n",
    "        \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:33.958795Z",
     "start_time": "2025-04-08T12:31:33.950374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "font_poisoning_mapping_df = pd.read_csv(os.path.join(pwd,'font_poisoning_mapping.csv'))\n",
    "\n",
    "def test_data_obfuscation(df, dl, dl_func, data_obfuscation_folder, input_format):\n",
    "    data_obfuscation_folder=os.path.join(data_obfuscation_folder,'Data obfuscation')\n",
    "    print(f'Data obfuscation for {dl}, {input_format} format')\n",
    "    for root,dirs,_ in os.walk(data_obfuscation_folder):\n",
    "            # print(dirs)\n",
    "            for dir_name in dirs:\n",
    "                if dir_name=='OCR-poisoning_default':\n",
    "                    continue\n",
    "                if dir_name!='Font-poisoning_default':\n",
    "                    continue\n",
    "                print(f'    dir_name={dir_name}')\n",
    "                # Get attack name, technique, and files\n",
    "                attack, technique= dir_name.split('_')\n",
    "                dir_path=os.path.abspath(os.path.join(root,dir_name))\n",
    "                files=[os.path.join(dir_path,f) for f in os.listdir(dir_path)]\n",
    "                # print(f'    attack: {attack}, technique: {technique}')\n",
    "                records_to_add=[]\n",
    "                for file in files:\n",
    "                    # Get word that has been obfuscated\n",
    "                    obfuscated_word=os.path.basename(os.path.basename(file)).split('.')[0].split('_')[-1]\n",
    "                    # Extract text\n",
    "                    extracted_text=dl_func(file)\n",
    "                    # Compute text result\n",
    "                    if dir_name=='Font-poisoning_default':\n",
    "                        if file not in font_poisoning_mapping_df['full_file_path'].values:\n",
    "                            test_result=\"NULL, FILE MAPPING WITH ORIGINAL TEXT FAILED\"\n",
    "                        else:\n",
    "                            original_text=font_poisoning_mapping_df[font_poisoning_mapping_df['full_file_path']==file]['joint_text'].values[0]\n",
    "                            test_result=cer(original_text,extracted_text)\n",
    "                    else:    \n",
    "                        test_result='Passed' if obfuscated_word not in extracted_text else 'Failed'\n",
    "                    # Add new record to dataframe\n",
    "                    records_to_add.append({\n",
    "                        'FileFormat': input_format,\n",
    "                        'Filename': os.path.basename(file),\n",
    "                        'AttackFamily': 'Data Obfuscation',\n",
    "                        'AttackName': attack,\n",
    "                        'Technique': technique,\n",
    "                        'DocumentLoader': dl,\n",
    "                        'AttackResult': test_result,\n",
    "                        'Text_extracted': extracted_text\n",
    "                    })\n",
    "                # Add records to dataframe\n",
    "                df=pd.concat([df,pd.DataFrame.from_records(records_to_add)])  \n",
    "              \n",
    "    return df\n",
    "\n",
    "def test_text_injection(df, dl, dl_func, text_injection_folder, input_format):\n",
    "    text_injection_folder=os.path.join(text_injection_folder,'Poisoned text injection')\n",
    "    print(f'Text injection for {dl}, {input_format} format')\n",
    "    for root,dirs,_ in os.walk(text_injection_folder):\n",
    "        # print(dirs)\n",
    "        for dir_name in dirs:\n",
    "            print(f'    dir_name={dir_name}')                \n",
    "            ## FOR NOW, SKIP METADATA!!!\n",
    "            if 'Metadata' in dir_name:\n",
    "                attack, technique= dir_name.split('_')\n",
    "                dir_path=os.path.abspath(os.path.join(root,dir_name))\n",
    "                files=[os.path.join(dir_path,f) for f in os.listdir(dir_path)]\n",
    "                # print(f'    attack: {attack}, technique: {technique}')\n",
    "                records_to_add=[]\n",
    "                for file in files:\n",
    "                    # Get word that has been obfuscated\n",
    "                    injected_word=os.path.basename(os.path.basename(file)).split('.')[0].split('_')[-1]\n",
    "                    # Extract text\n",
    "\n",
    "                    extracted_text=dl_func_mapping_metadata[dl](file)\n",
    "                    normalized_text=extracted_text.lower()\n",
    "                    # Generate possible author strings\n",
    "                    possible_keys = [\n",
    "                        f'author : {injected_word.lower()}',\n",
    "                        f'/author : {injected_word.lower()}'\n",
    "                    ]\n",
    "\n",
    "                    # Check if any of the keys are in the normalized text\n",
    "                    if any(key in normalized_text for key in possible_keys):\n",
    "                        test_result = 'Passed'\n",
    "                    else:\n",
    "                        test_result = 'Failed'\n",
    "\n",
    "                    # Add new record to dataframe\n",
    "                    records_to_add.append({\n",
    "                        'FileFormat': input_format,\n",
    "                        'Filename': os.path.basename(file),\n",
    "                        'AttackFamily': 'Text injection',\n",
    "                        'AttackName': attack,\n",
    "                        'Technique': technique,\n",
    "                        'DocumentLoader': dl,\n",
    "                        'AttackResult': test_result,\n",
    "                        'Text_extracted': extracted_text\n",
    "                    })\n",
    "\n",
    "                # Add records to dataframe\n",
    "                df=pd.concat([df,pd.DataFrame.from_records(records_to_add)])\n",
    "                continue\n",
    "            ## ELSE\n",
    "            # Get attack name, technique, and files\n",
    "            attack, technique= dir_name.split('_')\n",
    "            dir_path=os.path.abspath(os.path.join(root,dir_name))\n",
    "            files=[os.path.join(dir_path,f) for f in os.listdir(dir_path)]\n",
    "            # print(f'    attack: {attack}, technique: {technique}')\n",
    "            records_to_add=[]\n",
    "            for file in files:\n",
    "                # Get word that has been obfuscated\n",
    "                injected_word=os.path.basename(os.path.basename(file)).split('.')[0].split('_')[-1]\n",
    "                # Extract text\n",
    "                extracted_text=dl_func(file)\n",
    "                # Compute text result\n",
    "                test_result='Passed' if injected_word in extracted_text else 'Failed'\n",
    "                # Add new record to dataframe\n",
    "                records_to_add.append({\n",
    "                    'FileFormat': input_format,\n",
    "                    'Filename': os.path.basename(file),\n",
    "                    'AttackFamily': 'Text injection',\n",
    "                    'AttackName': attack,\n",
    "                    'Technique': technique,\n",
    "                    'DocumentLoader': dl,\n",
    "                    'AttackResult': test_result,\n",
    "                    'Text_extracted': extracted_text\n",
    "                })\n",
    "            \n",
    "            # Add records to dataframe\n",
    "            df=pd.concat([df,pd.DataFrame.from_records(records_to_add)])   \n",
    "    return df\n",
    "\n",
    "def general_testing(df, dl_to_test, dl_func_mapping, dataset_path, df_output_path):\n",
    "    \n",
    "    for dl in tqdm(dl_to_test, desc=f\"Testing document loaders\"):\n",
    "        # print(f\"Testing document loader: {dl}\")    \n",
    "        # Get function to extract test\n",
    "        dl_func=dl_func_mapping[dl]\n",
    "\n",
    "        input_format_to_test=dl_to_test[dl]\n",
    "\n",
    "        # get DOCX,PDF AND webpages folder from dataset path\n",
    "        docx_folder=os.path.join(dataset_path,'DOCX')\n",
    "        pdf_folder=os.path.join(dataset_path,'PDF')\n",
    "        html_folder=os.path.join(dataset_path,'HTML')\n",
    "        print(f'pdf_folder={pdf_folder}\\ndocx_folder={docx_folder}\\nhtml_folder={html_folder}')\n",
    "        if 'PDF' in input_format_to_test:\n",
    "            df=test_data_obfuscation(df, dl, dl_func, pdf_folder,'PDF')\n",
    "            # df=test_text_injection(df, dl, dl_func, pdf_folder,'PDF')\n",
    "            # df.to_csv(df_output_path,index=False)\n",
    "        if 'DOCX' in input_format_to_test:\n",
    "            df=test_data_obfuscation(df, dl, dl_func, docx_folder,\"DOCX\")\n",
    "            # df=test_text_injection(df, dl, dl_func, docx_folder,\"DOCX\")\n",
    "            # df.to_csv(df_output_path,index=False)\n",
    "        if 'HTML' in input_format_to_test:\n",
    "            df=test_data_obfuscation(df, dl, dl_func, html_folder,\"HTML\")\n",
    "            # df=test_text_injection(df, dl, dl_func, html_folder,\"HTML\")\n",
    "            # df.to_csv(df_output_path,index=False)\n",
    "        df.to_csv(df_output_path,index=False)\n",
    "    return df "
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:31:39.528244Z",
     "start_time": "2025-04-08T12:31:39.523385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "DATASET_PATH=os.path.join(pwd,'data/dataset')\n",
    "DF_OUTPUT_PATH=os.path.join(pwd,'local_goal_testing.csv')\n",
    "# General dataset creation / loading\n",
    "is_dataset_created = False\n",
    "dataset_path = DF_OUTPUT_PATH\n",
    "\n",
    "if is_dataset_created:\n",
    "    \n",
    "    df = pd.read_csv(dataset_path)\n",
    "else:\n",
    "    df=pd.DataFrame(columns=['FileFormat','Filename', 'AttackFamily','AttackName','Technique','DocumentLoader','AttackResult','Text_extracted'])\n",
    "\n",
    "# Document loaders to be tested\n",
    "# key: document loader, value = input_format_to_test\n",
    "#NB: Add corresponding function + function mapping every new document loader\n",
    "document_loaders_to_test={\n",
    "    'docling_default': ['PDF','DOCX','HTML'],\n",
    "    'docling_pyPDFium': ['PDF'],\n",
    "    'docling_DOCX': ['DOCX'],\n",
    "    'docling_HTML': ['HTML'],\n",
    "    'haystack_pyPDF': ['PDF'],\n",
    "    'haystack_PDFMiner': ['PDF'],\n",
    "    'haystack_docx': ['DOCX'],\n",
    "    'haystack_html': ['HTML'],\n",
    "    'llamaindex_llamaparse': ['PDF','DOCX','HTML'],\n",
    "    'llamaindex_simpleDirectoryReader' : ['PDF','DOCX'],\n",
    "    'llamaindex_HTMLTagReader': ['HTML'],\n",
    "\n",
    "    # # Add more document loaders\n",
    "    'langchain_pyPDF':['PDF'],\n",
    "    'langchain_PDFPlumber':['PDF'],\n",
    "    'langchain_PyPDFium2':['PDF'],\n",
    "    'langchain_PyMUPDF':['PDF'],\n",
    "    'langchain_Docx2txt':['DOCX'],\n",
    "    'langchain_UnstructuredHTML':['HTML'], #DONE!\n",
    "    'langchain_BSHtml':['HTML'], #DONE!\n",
    "    'llmsherpa_default':['PDF','DOCX','HTML'],\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "df = general_testing(df, document_loaders_to_test, dl_func_mapping, DATASET_PATH, DF_OUTPUT_PATH)\n"
   ],
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
