{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:44.825793Z",
     "start_time": "2025-04-07T16:31:44.821468Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "import requests\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from docx.shared import Pt, RGBColor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:44.841515Z",
     "start_time": "2025-04-07T16:31:44.839888Z"
    }
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:56.472856Z",
     "start_time": "2025-04-07T16:31:44.893074Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Grocery_and_Gourmet_Food\", streaming=True, trust_remote_code=True)\n",
    "print(type(dataset))\n",
    "splits = dataset.keys()\n",
    "#print all splits\n",
    "print(splits)\n",
    "\n",
    "for example in dataset[\"full\"]:\n",
    "    print(example)\n",
    "    break  # Remove this line to process the entire dataset\n",
    "\n",
    "dataset_iterator = iter(dataset[\"full\"])\n",
    "#get example[title]\n",
    "print(next(dataset_iterator)[\"title\"])\n",
    "#get length of dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:56.481662Z",
     "start_time": "2025-04-07T16:31:56.478957Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "pwd = os.path.dirname(os.getcwd())\n",
    "print(pwd)\n",
    "font_path = os.path.join(pwd, 'DejaVuSans.ttf')\n",
    "dataset_path = os.path.join(pwd, \"data\" ,\"dataset\", \"DOCX\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)\n",
    "else:\n",
    "    print(\"Folder already exists, deleting and creating new one\")\n",
    "    shutil.rmtree(dataset_path)\n",
    "    os.makedirs(dataset_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:56.526670Z",
     "start_time": "2025-04-07T16:31:56.523663Z"
    }
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "\n",
    "def create_docx_with_font(output_docx, title, body):#custom font not supported\n",
    "    \"\"\"\n",
    "    Creates a DOCX with the specified title and body content, formatting numbered lists appropriately.\n",
    "\n",
    "    Args:\n",
    "        output_docx (str): Path to save the output DOCX file.\n",
    "        title (str): The title of the document.\n",
    "        body (str): The body text of the document. Lines starting with \"1.\", \"2.\", etc., will be formatted as a numbered list.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize DOCX\n",
    "    doc = Document()\n",
    "    font_name=\"DejaVu Sans\"\n",
    "    # Add title\n",
    "    title_paragraph = doc.add_heading(level=1)\n",
    "    run = title_paragraph.add_run(title)\n",
    "    run.font.size = Pt(16)\n",
    "    run.font.name = font_name\n",
    "    r = run._element\n",
    "    rPr = r.get_or_add_rPr()\n",
    "    rFonts = OxmlElement('w:rFonts')\n",
    "    rFonts.set(qn('w:eastAsia'), font_name)\n",
    "    rPr.append(rFonts)\n",
    "\n",
    "    # Add body text\n",
    "    doc.add_paragraph()  # Line break after title\n",
    "    lines = body.splitlines()  # Split body into lines\n",
    "    for line in lines:\n",
    "        if line.strip().startswith((\"1.\", \"2.\", \"3.\", \"4.\", \"5.\", \"6.\", \"7.\", \"8.\", \"9.\")):\n",
    "            # Indent numbered list items\n",
    "            paragraph = doc.add_paragraph(style='List Number')\n",
    "            run = paragraph.add_run(line.strip())\n",
    "            run.font.size = Pt(12)\n",
    "            run.font.name = font_name\n",
    "            r = run._element\n",
    "            rPr = r.get_or_add_rPr()\n",
    "            rFonts = OxmlElement('w:rFonts')\n",
    "            rFonts.set(qn('w:eastAsia'), font_name)\n",
    "            rPr.append(rFonts)\n",
    "        else:\n",
    "            # Regular paragraph\n",
    "            paragraph = doc.add_paragraph()\n",
    "            run = paragraph.add_run(line.strip())\n",
    "            run.font.size = Pt(12)\n",
    "            run.font.name = font_name\n",
    "            r = run._element\n",
    "            rPr = r.get_or_add_rPr()\n",
    "            rFonts = OxmlElement('w:rFonts')\n",
    "            rFonts.set(qn('w:eastAsia'), font_name)\n",
    "            rPr.append(rFonts)\n",
    "\n",
    "    # Save to file\n",
    "    doc.save(output_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:56.577963Z",
     "start_time": "2025-04-07T16:31:56.572088Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import math\n",
    "\n",
    "def select_target_word(text):\n",
    "    words = re.split(r'\\W+', text)  # Split by any non-word characters\n",
    "    words = [word for word in words if word.isalpha()]  # Keep only words with alphabetic characters\n",
    "    \n",
    "    if not words:\n",
    "        raise ValueError(\"No valid candidate words found.\")\n",
    "    \n",
    "    valid_words = [word for word in words if len(word) >= 2]  # Filter words by minimum length of 2 characters\n",
    "    \n",
    "    if not valid_words:\n",
    "        raise ValueError(\"No valid candidate words found with the required length.\")\n",
    "    \n",
    "    target_word = random.choice(valid_words)\n",
    "    return target_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Width Characters injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:56.630841Z",
     "start_time": "2025-04-07T16:31:56.623163Z"
    }
   },
   "outputs": [],
   "source": [
    "class ZeroWidthSPaceAttack:\n",
    "    '''define list of malicious characters'''\n",
    "    def __init__(self):\n",
    "        #define the list of malicious symbols\n",
    "        self.symbols = [u'\\u200b', u'\\u200c', u'\\u200d', u'\\u200e', u'\\u200f', #U+200x\n",
    "                      u'\\u202a', u'\\u202b', u'\\u202c', u'\\u202d', #U+202x\n",
    "                      u'\\u2060', u'\\u2061', u'\\u2062', u'\\u2063', u'\\u2064',\n",
    "                    #   u'\\u2065', u'\\u2066', u'\\u2067', u'\\u2068', u'\\u2069', NOT SUPPORTED BY FONT\n",
    "                      u'\\u206a', u'\\u206b', u'\\u206c', u'\\u206d', u'\\u206e' #U+206x\n",
    "                      ]\n",
    "        self.num_malicius_chars = len(self.symbols)\n",
    "\n",
    "    '''insertion of malicious input in the middle of a given word;\n",
    "        for example, given the word \"love\", the result is \"loXve\".\n",
    "    '''\n",
    "    def mask1(self, word, index = 0, random = True):\n",
    "        '''word = target word \\n index = index of malicious symbols from the\n",
    "        given list\\nrandom = if random True, a randomic index is selected\\n'''\n",
    "\n",
    "        #check the index\n",
    "        if index < 0 or index > len(self.symbols):\n",
    "            raise Exception(\"Invalid index.\")\n",
    "\n",
    "        #sample the index, if required\n",
    "        if random:\n",
    "            index = np.random.randint(len(self.symbols))\n",
    "\n",
    "        #get the target character\n",
    "        code = self.symbols[index]\n",
    "\n",
    "        #prepare the result. it must to be unicode\n",
    "        poison = []\n",
    "\n",
    "        #calculate the middle of the word\n",
    "        mid = len(word) // 2\n",
    "\n",
    "        #create the final message\n",
    "        poison.append(word[:mid])\n",
    "        poison.append(code)\n",
    "        poison.append(word[mid:])\n",
    "\n",
    "        poison = ''.join(poison)\n",
    "\n",
    "        return poison\n",
    "\n",
    "    '''insertion of malicious input between each character of the word;\n",
    "\n",
    "        for example, given the word \"love\", the result is \"lXoXvXe\".\n",
    "    '''\n",
    "    def mask2(self, word, index = 0, random = True):\n",
    "        '''word = target word\\nindex = index of malicious symbols from the\n",
    "        given list\\nrandom = if random True, a randomic index is selected\\n'''\n",
    "\n",
    "        #check the index\n",
    "        if index < 0 or index > len(self.symbols):\n",
    "            raise Exception(\"Invalid index.\")\n",
    "\n",
    "        #sample the index, if required\n",
    "        if random:\n",
    "            index = np.random.randint(len(self.symbols))\n",
    "\n",
    "        #get the target character\n",
    "        code = self.symbols[index]\n",
    "\n",
    "        #prepare the result. it must to be unicode\n",
    "        poison = []\n",
    "\n",
    "        #calculate the middle of the word\n",
    "        mid = len(word) // 2\n",
    "\n",
    "        #create the final message\n",
    "        poison.append(word[:mid])\n",
    "        # For now 3, in the midde\n",
    "        poison.append(code)\n",
    "        poison.append(code)\n",
    "        poison.append(code)\n",
    "        \n",
    "        poison.append(word[mid:])\n",
    "\n",
    "        return ''.join(poison)\n",
    "\n",
    "    ''' define a function that remove Zero-Width SPace (ZWSP) characters '''\n",
    "    def sanitization(self, sentence):\n",
    "        #blacklist characters removal\n",
    "        res = ''.join([c for c in sentence if c not in self.symbols])\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:56.685014Z",
     "start_time": "2025-04-07T16:31:56.677602Z"
    }
   },
   "outputs": [],
   "source": [
    "''' define a function that remove Zero-Width SPace (ZWSP) characters '''\n",
    "def create_zew_mask1_docx(output_path_attak_method, title, body, id):\n",
    "    \n",
    "    zew=ZeroWidthSPaceAttack()\n",
    "    symbols_len=zew.num_malicius_chars\n",
    "    \n",
    "    #set the target word random form body\n",
    "    target_word=select_target_word(body)\n",
    "    #select random index\n",
    "    i_symbol = np.random.randint(symbols_len)\n",
    "\n",
    "    poisoned_title = title.replace(target_word,zew.mask1(target_word,i_symbol,random=False))\n",
    "    poisoned_body = body.replace(target_word,zew.mask1(target_word,i_symbol,random=False))\n",
    "    symbol_string=f\"u{ord(zew.symbols[i_symbol]):04X}\"\n",
    "    path_docx = os.path.join(output_path_attak_method, f\"{id}_{symbol_string}_{target_word}.docx\")\n",
    "\n",
    "    create_docx_with_font(path_docx, poisoned_title, poisoned_body)\n",
    "    return path_docx\n",
    "\n",
    "def create_zew_mask2_docx(output_path_attak_method, title, body, id):\n",
    "    zew=ZeroWidthSPaceAttack()\n",
    "    symbols_len=zew.num_malicius_chars\n",
    "    \n",
    "    #set the target word random form body\n",
    "    target_word=select_target_word(body)\n",
    "    #select random index\n",
    "    i_symbol = np.random.randint(symbols_len)\n",
    "\n",
    "    poisoned_title = title.replace(target_word,zew.mask2(target_word,i_symbol,random=False))\n",
    "    poisoned_body = body.replace(target_word,zew.mask2(target_word,i_symbol,random=False))\n",
    "    symbol_string=f\"u{ord(zew.symbols[i_symbol]):04X}\"\n",
    "    path_docx = os.path.join(output_path_attak_method, f\"{id}_{symbol_string}_{target_word}.docx\")\n",
    "\n",
    "    create_docx_with_font(path_docx, poisoned_title, poisoned_body)\n",
    "    return path_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:56.736869Z",
     "start_time": "2025-04-07T16:31:56.730806Z"
    }
   },
   "outputs": [],
   "source": [
    "#test funtions\n",
    "if DEBUG:\n",
    "    title = \"i love drugs\"\n",
    "    body = \"i love drugs\"\n",
    "\n",
    "    dataset_obf= os.path.join(dataset_path, \"Data obfuscation\")\n",
    "    dataset_mask1 = os.path.join(dataset_obf, \"Zero-Width_Mask1\")\n",
    "    dataset_mask2 = os.path.join(dataset_obf, \"Zero-Width_Mask2\")\n",
    "    if not os.path.exists(dataset_mask1):\n",
    "        os.makedirs(dataset_mask1)\n",
    "    if not os.path.exists(dataset_mask2):\n",
    "        os.makedirs(dataset_mask2)\n",
    "    print(create_zew_mask1_docx(dataset_mask1, title, body, 1))\n",
    "    print(create_zew_mask2_docx(dataset_mask2, title, body, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homoglyph Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:56.789199Z",
     "start_time": "2025-04-07T16:31:56.783249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to replace characters with Unicode homoglyphs\n",
    "def replace_text_with_homoglyphs(text, homoglyphs):\n",
    "    output=\"\"\n",
    "    for i in text:\n",
    "        if i in homoglyphs:\n",
    "            output += random.choice(homoglyphs[i]) \n",
    "        else:\n",
    "            output += i\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.057723Z",
     "start_time": "2025-04-07T16:31:56.837337Z"
    }
   },
   "outputs": [],
   "source": [
    "confusables = dict()\n",
    "intentionals = dict()\n",
    "\n",
    "# Retrieve Unicode Confusable homoglyph characters\n",
    "conf_resp = requests.get(\"https://www.unicode.org/Public/security/latest/confusables.txt\", stream=True)\n",
    "for line in conf_resp.iter_lines():\n",
    "  if len(line):\n",
    "    line = line.decode('utf-8-sig')\n",
    "    if line[0] != '#':\n",
    "      line = line.replace(\"#*\", \"#\")\n",
    "      _, line = line.split(\"#\", maxsplit=1)\n",
    "      if line[3] not in confusables:\n",
    "        confusables[line[3]] = []\n",
    "      confusables[line[3]].append(line[7])\n",
    "\n",
    "# Retrieve Unicode Intentional homoglyph characters\n",
    "int_resp = requests.get(\"https://www.unicode.org/Public/security/latest/intentional.txt\", stream=True)\n",
    "for line in int_resp.iter_lines():\n",
    "  if len(line):\n",
    "    line = line.decode('utf-8-sig')\n",
    "    if line[0] != '#':\n",
    "      line = line.replace(\"#*\", \"#\")\n",
    "      _, line = line.split(\"#\", maxsplit=1)\n",
    "      if line[3] not in intentionals:\n",
    "        intentionals[line[3]] = []\n",
    "      intentionals[line[3]].append(line[7])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.073542Z",
     "start_time": "2025-04-07T16:31:58.069587Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_homoglyph_docx(output_path_attak_method, title, body, id):\n",
    "    path= output_path_attak_method\n",
    "    #set the target word random form body\n",
    "    target_word=select_target_word(body)\n",
    "    obfuscated_word=replace_text_with_homoglyphs(target_word,intentionals)\n",
    "    while(obfuscated_word==target_word):\n",
    "       print(\"Homoglyphs, same obfuscated word generated\")\n",
    "       target_word=select_target_word(body)\n",
    "       obfuscated_word=replace_text_with_homoglyphs(target_word,intentionals)\n",
    "\n",
    "    # Replace the target word with a homoglyph\n",
    "    poisoned_title=title.replace(target_word,obfuscated_word)\n",
    "    poisoned_body=body.replace(target_word,obfuscated_word)\n",
    "    path_docx = os.path.join(path, f\"{id}_{target_word}.docx\")\n",
    "\n",
    "    create_docx_with_font(path_docx, poisoned_title, poisoned_body)\n",
    "    return path_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.129212Z",
     "start_time": "2025-04-07T16:31:58.127123Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    homoglyph_path = os.path.join(dataset_obf, \"Homoglyph\")\n",
    "    if not os.path.exists(homoglyph_path):\n",
    "        os.makedirs(homoglyph_path)\n",
    "    create_homoglyph_docx(homoglyph_path, title, body, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.183553Z",
     "start_time": "2025-04-07T16:31:58.173654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unicode Bidi override characters\n",
    "PDF = chr(0x202C)\n",
    "LRE = chr(0x202A)\n",
    "RLE = chr(0x202B)\n",
    "LRO = chr(0x202D)\n",
    "RLO = chr(0x202E)\n",
    "\n",
    "PDI = chr(0x2069)\n",
    "LRI = chr(0x2066)\n",
    "RLI = chr(0x2067)\n",
    "\n",
    "# Class to apply Unicode Bidi override characters to obfuscate text\n",
    "class Swap():\n",
    "    \"\"\"Represents swapped elements in a string of text.\"\"\"\n",
    "    def __init__(self, one, two):\n",
    "        self.one = one\n",
    "        self.two = two\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Swap({self.one}, {self.two})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.one == other.one and self.two == other.two\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.one, self.two))\n",
    "\n",
    "def some(*els):\n",
    "    \"\"\"Returns the arguments as a tuple with Nones removed.\"\"\"\n",
    "    return tuple(filter(None, tuple(els)))\n",
    "\n",
    "def swaps(chars: str) -> set:\n",
    "    \"\"\"Generates all possible swaps for a string.\"\"\"\n",
    "    def pairs(chars, pre=(), suf=()):\n",
    "        orders = set()\n",
    "        for i in range(len(chars)-1):\n",
    "            prefix = pre + tuple(chars[:i])\n",
    "            suffix = suf + tuple(chars[i+2:])\n",
    "            swap = Swap(chars[i+1], chars[i])\n",
    "            pair = some(prefix, swap, suffix)\n",
    "            orders.add(pair)\n",
    "            orders.update(pairs(suffix, pre=some(prefix, swap)))\n",
    "            orders.update(pairs(some(prefix, swap), suf=suffix))\n",
    "        return orders\n",
    "    return pairs(chars) | {tuple(chars)}\n",
    "\n",
    "def unswap(el: tuple) -> str:\n",
    "    \"\"\"Reverts a tuple of swaps to the original string.\"\"\"\n",
    "    if isinstance(el, str):\n",
    "        return el\n",
    "    elif isinstance(el, Swap):\n",
    "        return unswap((el.two, el.one))\n",
    "    else:\n",
    "        res = \"\"\n",
    "        for e in el:\n",
    "            res += unswap(e)\n",
    "        return res\n",
    "\n",
    "def uniswap(els):\n",
    "    res = \"\"\n",
    "    for el in els:\n",
    "        if isinstance(el, Swap):\n",
    "            res += uniswap([LRO, LRI, RLO, LRI, el.one, PDI, LRI, el.two, PDI, PDF, PDI, PDF])\n",
    "        elif isinstance(el, str):\n",
    "            res += el\n",
    "        else:\n",
    "            for subel in el:\n",
    "                res += uniswap([subel])\n",
    "    return res\n",
    "\n",
    "def strings_to_file(file, string):\n",
    "  with open(file, 'w') as f:\n",
    "      for swap in swaps(string):\n",
    "          uni = uniswap(swap)\n",
    "          print(uni, file=f)\n",
    "\n",
    "def print_strings(string):\n",
    "  for swap in swaps(string):\n",
    "    uni = uniswap(swap)\n",
    "    print(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.237326Z",
     "start_time": "2025-04-07T16:31:58.229613Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_reordering_attack_docx(output_path_attak_method, title, body, id):\n",
    "\n",
    "    target_word=select_target_word(body)\n",
    "    swapped_words =swaps(target_word)\n",
    "\n",
    "    obfuscated_words=[uniswap(swap) for swap in swapped_words]\n",
    "    #select random obfuscated word\n",
    "    obfuscated_word = obfuscated_words[np.random.randint(len(obfuscated_words))]\n",
    "    path_docx = os.path.join(output_path_attak_method, f\"{id}_{target_word}.docx\")\n",
    "\n",
    "    create_docx_with_font(path_docx, title.replace(target_word,obfuscated_word), body.replace(target_word,obfuscated_word))\n",
    "    return path_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.288725Z",
     "start_time": "2025-04-07T16:31:58.284015Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    reorder_path = os.path.join(dataset_obf, \"Reordering_attacks\")\n",
    "    if not os.path.exists(reorder_path):\n",
    "        os.makedirs(reorder_path)\n",
    "    create_reordering_attack_docx(reorder_path, title, body, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camouflage Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.341042Z",
     "start_time": "2025-04-07T16:31:58.335309Z"
    }
   },
   "outputs": [],
   "source": [
    "def has_diacritical_accent(text):\n",
    "    diacritical_accents = \"àèéìòùÀÈÉÌÒÙ\"\n",
    "    return any(char in diacritical_accents for char in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.400443Z",
     "start_time": "2025-04-07T16:31:58.388926Z"
    }
   },
   "outputs": [],
   "source": [
    "def obfuscate_diacritical(word, times = 10):\n",
    "    diacritical_accents = \"àèéìòùÀÈÉÌÒÙ\"\n",
    "    #chek witch diacritical accent is in the word\n",
    "\n",
    "    for i,char in enumerate(word):\n",
    "        if char in diacritical_accents:\n",
    "            if char in (\"à\", \"è\", \"ì\", \"ò\", \"ù\", \"À\", \"È\", \"É\", \"Ì\", \"Ò\", \"Ù\"):\n",
    "                word = word[:i+1] + '\\u0300'*times + word[i+1:]\n",
    "\n",
    "            else:\n",
    "                word = word[:i+1] + '\\u0301'*times + word[i+1:]\n",
    "    return word\n",
    "   \n",
    "    #replace the accent with the unicode character\n",
    "diacritical_accents = \"àèéìòùÀÈÉÌÒÙ\"\n",
    "\n",
    "def select_target_diacritical_word(text):\n",
    "    words = re.split(r'\\W+', text)  # Split by any non-word characters\n",
    "    words = [word for word in words if word.isalpha()]  # Keep only words with alphabetic characters\n",
    "    \n",
    "    if not words:\n",
    "        raise ValueError(\"No valid candidate words found.\")\n",
    "    \n",
    "    valid_words = [word for word in words if len(word) >= 2]  # Filter words by minimum length of 2 characters\n",
    "    \n",
    "    if not valid_words:\n",
    "        raise ValueError(\"No valid candidate words found with the required length.\")\n",
    "    \n",
    "    diacritical_words = [word for word in words if any(char in diacritical_accents for char in word)]\n",
    "    \n",
    "    if diacritical_words:\n",
    "        target_word = random.choice(diacritical_words)\n",
    "    else:\n",
    "        #rise an exception\n",
    "        raise ValueError(\"No valid candidate diacritial words found.\")\n",
    "    \n",
    "    return target_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.446158Z",
     "start_time": "2025-04-07T16:31:58.442778Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_diactricial_marks_injection_mask1_docx(output_path_attak_method, title, body, id):\n",
    "    path = output_path_attak_method\n",
    "    #set the target word random form body\n",
    "    target_word=select_target_word(body)\n",
    "    #select random obfuscated word among word with accent\n",
    "    \n",
    "    target_word = select_target_diacritical_word(body)\n",
    "    obfuscated_word = obfuscate_diacritical(target_word)\n",
    "\n",
    "    \n",
    "\n",
    "    poisoned_title=title.replace(target_word,obfuscated_word)\n",
    "    poisoned_body=body.replace(target_word,obfuscated_word)\n",
    "    \n",
    "    path_docx = os.path.join(path, f\"{id}_{target_word}.docx\")\n",
    "    create_docx_with_font(path_docx, poisoned_title, poisoned_body)\n",
    "\n",
    "    return path_docx\n",
    "\n",
    "def create_diactricial_marks_injection_mask2_docx(output_path_attak_method, title, body, id):\n",
    "    path = output_path_attak_method\n",
    "    #set the target word random form body\n",
    "    target_word=select_target_word(body)\n",
    "    #select random obfuscated word among word with accents\n",
    "        \n",
    "    target_word = select_target_diacritical_word(body)\n",
    "    obfuscated_word = obfuscate_diacritical(target_word, 4096**2*2)\n",
    "\n",
    "    \n",
    "\n",
    "    poisoned_title=title.replace(target_word,obfuscated_word)\n",
    "    poisoned_body=body.replace(target_word,obfuscated_word)\n",
    "    \n",
    "    path_docx = os.path.join(path, f\"{id}_{target_word}.docx\")\n",
    "    create_docx_with_font(path_docx, poisoned_title, poisoned_body)\n",
    "\n",
    "    return path_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.498913Z",
     "start_time": "2025-04-07T16:31:58.492424Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    diacritical_mask1_path = os.path.join(dataset_obf, \"Diactricial_marks_injection_mask1\")\n",
    "    if not os.path.exists(diacritical_mask1_path):\n",
    "        os.makedirs(diacritical_mask1_path)\n",
    "    diacritical_mask2_path = os.path.join(dataset_obf, \"Diactricial_marks_injection_mask2\")\n",
    "    if not os.path.exists(diacritical_mask2_path):\n",
    "        os.makedirs(diacritical_mask2_path)\n",
    "    title = \"i love drùgs\"\n",
    "    body = \"i love drùgs and cofè\"\n",
    "    create_diactricial_marks_injection_mask1_docx(diacritical_mask1_path, title, body, 1)\n",
    "    create_diactricial_marks_injection_mask2_docx(diacritical_mask2_path, title, body, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Injecting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.643120Z",
     "start_time": "2025-04-07T16:31:58.545272Z"
    }
   },
   "outputs": [],
   "source": [
    "words_df=pd.read_csv('unigram_freq.csv')\n",
    "lowest_freq_words=words_df.sort_values('count',ascending=True).head(2000)['word'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.650867Z",
     "start_time": "2025-04-07T16:31:58.649343Z"
    }
   },
   "outputs": [],
   "source": [
    "words_current_index = 0\n",
    "\n",
    "def get_next_word():\n",
    "    global words_current_index\n",
    "    word = lowest_freq_words[words_current_index]\n",
    "    words_current_index = (words_current_index + 1) % len(lowest_freq_words)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.714635Z",
     "start_time": "2025-04-07T16:31:58.711624Z"
    }
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Pt, RGBColor\n",
    "\n",
    "def add_hidden_text_docx(path_docx, hidden_text, font_name=\"DejaVu Sans\"):\n",
    "    # Initialize DOCX\n",
    "    doc = Document(path_docx)\n",
    "\n",
    "    # Add hidden text (white text matching background)\n",
    "    hidden_paragraph = doc.add_paragraph()\n",
    "    hidden_run = hidden_paragraph.add_run(hidden_text)\n",
    "    hidden_run.font.size = Pt(12)\n",
    "    hidden_run.font.name = font_name\n",
    "    hidden_run.font.color.rgb = RGBColor(255, 255, 255)  # White color\n",
    "\n",
    "    # Save the DOCX\n",
    "    doc.save(path_docx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.765452Z",
     "start_time": "2025-04-07T16:31:58.763609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to add hidden text to an already existing DOCX file\n",
    "def add_vanished_text_to_docx(path_docx, hidden_text):\n",
    "    # Load the existing DOCX file\n",
    "    doc = Document(path_docx)\n",
    "\n",
    "    # Create a new paragraph with hidden text\n",
    "    p = doc.add_paragraph()\n",
    "    run = p.add_run(hidden_text)\n",
    "\n",
    "    # Set the text to be hidden\n",
    "    rPr = run._r.get_or_add_rPr()\n",
    "    vanishing = OxmlElement('w:vanish')\n",
    "    rPr.append(vanishing)\n",
    "\n",
    "    # Save the modified DOCX to the output path\n",
    "    doc.save(path_docx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.820322Z",
     "start_time": "2025-04-07T16:31:58.812213Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_transparent_bg_docx(output_path_attak_method, title, body, id):\n",
    "    #set the target word random form body\n",
    "    background_color = \"transparent\"\n",
    "    tag_name=\"h1\"\n",
    "    hidden_word = get_next_word()\n",
    "    path_docx = os.path.join(output_path_attak_method, f\"{id}_{hidden_word}.docx\")\n",
    "\n",
    "\n",
    "    create_docx_with_font(path_docx, title, body)\n",
    "    add_hidden_text_docx(path_docx, title, hidden_word)\n",
    "    return path_docx\n",
    "def create_transparent_vanish_docx(output_path_attak_method, title, body, id):\n",
    "    hidden_word = get_next_word()\n",
    "    path_docx = os.path.join(output_path_attak_method, f\"{id}_{hidden_word}.docx\")\n",
    "\n",
    "    create_docx_with_font(path_docx, title, body)\n",
    "    add_vanished_text_to_docx(path_docx, hidden_word)\n",
    "    return path_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.875533Z",
     "start_time": "2025-04-07T16:31:58.867327Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    path_transparent_text_bg = os.path.join(dataset_injection, \"Transparent_Text_injection_background\")\n",
    "    if not os.path.exists(path_transparent_text_bg):\n",
    "        os.makedirs(path_transparent_text_bg)\n",
    "    path_transparent_text_opacity = os.path.join(dataset_injection, \"Transparent_Text_injection_opacity\")\n",
    "    if not os.path.exists(path_transparent_text_opacity):\n",
    "        os.makedirs(path_transparent_text_opacity)\n",
    "\n",
    "    title = \"i love drugs\"\n",
    "    body = \"i love drugs\"\n",
    "    create_transparent_bg_docx(path_transparent_text_bg, title, body, 1)\n",
    "    create_transparent_vanish_docx(path_transparent_text_opacity, title, body, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.928772Z",
     "start_time": "2025-04-07T16:31:58.922043Z"
    }
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "def add_outofmargin_text_to_docx(existing_docx_path, output_path, hidden_text, left, top):\n",
    "    # Load the existing DOCX file\n",
    "    doc = Document(existing_docx_path)\n",
    "\n",
    "    # Create a new text box with visible text\n",
    "    section = doc.sections[0]\n",
    "    new_width = section.page_width - Inches(left)\n",
    "    new_height = section.page_height - Inches(top)\n",
    "\n",
    "    # Add a new paragraph with the text\n",
    "    p = doc.add_paragraph()\n",
    "    run = p.add_run(hidden_text)\n",
    "\n",
    "    # Set the position of the text box\n",
    "    p_format = p.paragraph_format\n",
    "    p_format.left_indent = Inches(left)\n",
    "    p_format.space_before = Inches(top)\n",
    "    p.alignment = WD_PARAGRAPH_ALIGNMENT.RIGHT\n",
    "\n",
    "    # Save the modified DOCX to the output path\n",
    "    doc.save(output_path)\n",
    "\n",
    "\n",
    "def create_outofmargin_text_docx(output_path_attak_method, title, body, id):\n",
    "    hidden_word = get_next_word()\n",
    "    path_docx = os.path.join(output_path_attak_method, f\"{id}_{hidden_word}.docx\")\n",
    "\n",
    "    create_docx_with_font(path_docx, title, body)\n",
    "    add_outofmargin_text_to_docx(path_docx, path_docx, hidden_word, 21, 0)\n",
    "    return path_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:58.981908Z",
     "start_time": "2025-04-07T16:31:58.976173Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    path_out_of_margin = os.path.join(dataset_injection, \"Out_of_margin_injection\")\n",
    "    if not os.path.exists(path_out_of_margin):\n",
    "        os.makedirs(path_out_of_margin)\n",
    "    title = \"i love drugs\"\n",
    "    body = \"i love drugs\"\n",
    "    create_outofmargin_text_docx(path_out_of_margin, title, body, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:59.033844Z",
     "start_time": "2025-04-07T16:31:59.028848Z"
    }
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "\n",
    "def add_zero_size_text_to_docx(existing_docx_path, output_path, hidden_text, font_name=\"DejaVu Sans\"):\n",
    "    # Load the existing DOCX\n",
    "    doc = Document(existing_docx_path)\n",
    "\n",
    "    # Add hidden text (zero-size text)\n",
    "    hidden_paragraph = doc.add_paragraph()\n",
    "    hidden_run = hidden_paragraph.add_run(hidden_text)\n",
    "    hidden_run.font.size = Pt(0)  # Set font size to zero\n",
    "    hidden_run.font.name = font_name\n",
    "\n",
    "    # Save the updated DOCX\n",
    "    doc.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:59.085873Z",
     "start_time": "2025-04-07T16:31:59.080046Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_zero_size_text_font_docx(output_path_attak_method, title, body, id):\n",
    "    hidden_word = get_next_word()\n",
    "    path_docx = os.path.join(output_path_attak_method, f\"{id}_{hidden_word}.docx\")\n",
    "\n",
    "    create_docx_with_font(path_docx, title, body)\n",
    "    add_zero_size_text_to_docx(path_docx, path_docx, hidden_word)\n",
    "    return path_docx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:31:59.141130Z",
     "start_time": "2025-04-07T16:31:59.134587Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    path_zero_size_text_zerofont = os.path.join(dataset_injection, \"Zero-size_Text_injection_font0\")\n",
    "    if not os.path.exists(path_zero_size_text_zerofont):\n",
    "        os.makedirs(path_zero_size_text_zerofont)\n",
    "    create_zero_size_text_font_docx(path_zero_size_text_zerofont, title, body, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decive Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:32:00.368719Z",
     "start_time": "2025-04-07T16:31:59.187004Z"
    }
   },
   "outputs": [],
   "source": [
    "#restart dataset iterator\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Grocery_and_Gourmet_Food\", streaming=True, trust_remote_code=True)\n",
    "dataset_iterator = iter(dataset[\"full\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:32:00.376222Z",
     "start_time": "2025-04-07T16:32:00.374599Z"
    }
   },
   "outputs": [],
   "source": [
    "attack_methods_obfuscation = {\n",
    "    \"Zero-Width_mask1\": create_zew_mask1_docx,\n",
    "    \"Zero-Width_mask2\": create_zew_mask2_docx,\n",
    "    \"Homoglyph_default\": create_homoglyph_docx,\n",
    "    #\"Font poisoning\": font,\n",
    "    \"Reordering-attack_default\": create_reordering_attack_docx,\n",
    "    \"Diacritical-injection_mask1\": create_diactricial_marks_injection_mask1_docx, #Camouflage Element\n",
    "    #\"Diactricial marks injection-mask2\": create_diactricial_marks_injection_mask2_html, DDOS\n",
    "    #\"OCR-poisoning injection\": create_ocr_poisoning_html\n",
    "}\n",
    "\n",
    "attack_methods_injection = {\n",
    "    \"Transparent-Text-injection_background-color\": create_transparent_vanish_docx,\n",
    "    \"Transparent-Text-injection_vanish\": create_transparent_vanish_docx,\n",
    "    \"Out-of-bound-injection_default\": create_outofmargin_text_docx,\n",
    "    \"Zero-size-injection_font1\": create_zero_size_text_font_docx,\n",
    "    #\"Zero-size injection-with scaling\": create_zero_size_scaled_html,\n",
    "    #\"Deceived-element injection\": create_decived_html,\n",
    "    #\"Metadata-injection\": create_metadata_hidden_text_html\n",
    "}\n",
    "\n",
    "super_class_map = {\n",
    "    \"Data obfuscation\": attack_methods_obfuscation,\n",
    "    \"Poisoned text injection\": attack_methods_injection\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:32:00.420382Z",
     "start_time": "2025-04-07T16:32:00.418611Z"
    }
   },
   "outputs": [],
   "source": [
    "RECREATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:32:00.471758Z",
     "start_time": "2025-04-07T16:32:00.467771Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Ottieni il percorso corrente\n",
    "pwd = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Definisci il percorso della cartella dataset\n",
    "dataset_path = os.path.join(pwd, \"data\", \"dataset\", \"DOCX\")\n",
    "print(dataset_path)\n",
    "if RECREATE:\n",
    "    # Crea la cartella se non esiste\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    else:\n",
    "        # Elimina tutti i file e le directory nella cartella dataset\n",
    "        files = glob.glob(os.path.join(dataset_path, '*'))\n",
    "        for f in files:\n",
    "            if os.path.isfile(f):\n",
    "                os.remove(f)\n",
    "            elif os.path.isdir(f):\n",
    "                shutil.rmtree(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:32:00.522262Z",
     "start_time": "2025-04-07T16:32:00.518685Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_sanitized(dataset_iterator):\n",
    "    \n",
    "    data_element = next(dataset_iterator)\n",
    "    title = data_element[\"title\"]\n",
    "    text = data_element[\"text\"]\n",
    "    \n",
    "    # check with re. r'\\w+' if there is at least one word in the body\n",
    "    if not re.search(r'\\w+', text, re.IGNORECASE):\n",
    "        raise ValueError(\"No valid candidate words found. No words in the body\")\n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T16:32:48.655318Z",
     "start_time": "2025-04-07T16:32:00.575782Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "num_files = 100\n",
    "count = 0\n",
    "#crete the folders folder_names if not existing\n",
    "for super_class in super_class_map:\n",
    "    if not os.path.exists(os.path.join(dataset_path, super_class)):\n",
    "        os.makedirs(os.path.join(dataset_path, super_class))\n",
    "    else :\n",
    "        print(\"folder already exists\")\n",
    "        # #remove all files in the folder\n",
    "        shutil.rmtree(os.path.join(dataset_path, super_class))\n",
    "        os.makedirs(os.path.join(dataset_path, super_class))\n",
    "    for sub_class in super_class_map[super_class]:\n",
    "        if not os.path.exists(os.path.join(dataset_path, super_class, sub_class)):\n",
    "            os.makedirs(os.path.join(dataset_path, super_class, sub_class))\n",
    "            print(os.path.join(dataset_path, super_class, sub_class))\n",
    "            path_pdf = os.path.join(dataset_path, super_class, sub_class)\n",
    "        \n",
    "        #ocr\n",
    "        if (sub_class ==  \"OCR-poisoning_default\") :\n",
    "            ocr_mapping_df_path=\"../ocr_mapping.csv\"\n",
    "            #if file does not exist, create new df\n",
    "            if not os.path.exists(ocr_mapping_df_path):\n",
    "                ocr_df=pd.DataFrame(columns=[\"file\", \"full_file_path\",\"title\",'text','joint_text'])\n",
    "                ocr_df.to_csv(ocr_mapping_df_path, index=False)\n",
    "            else:\n",
    "                ocr_df=pd.read_csv(ocr_mapping_df_path)\n",
    "    \n",
    "            for i in range(num_files):    \n",
    "                while True:  # Loop to retry if an error occurs\n",
    "                    try:\n",
    "\n",
    "                        title, text = extract_sanitized(dataset_iterator)\n",
    "\n",
    "                        joint_text = title + \"\\n\\n\" + text\n",
    "                        \n",
    "\n",
    "                        creation = super_class_map[super_class][sub_class]\n",
    "                        path_pdf_file=creation(path_pdf, title, text, i)\n",
    "                        print(path_pdf_file)\n",
    "\n",
    "                        file_name=os.path.basename(path_pdf_file)\n",
    "\n",
    "                        ocr_df.loc[len(ocr_df)] = {\n",
    "                            \"file\": file_name,\n",
    "                            \"full_file_path\": path_pdf_file,\n",
    "                            \"title\": title,\n",
    "                            \"text\": text,\n",
    "                            \"joint_text\": joint_text\n",
    "                        }\n",
    "                        break  # Exit the loop if successful\n",
    "                    except StopIteration:\n",
    "                        print(\"No more elements in the dataset.\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error occurred: {e}. Retrying with the next dataset element..., cout error {count}\")\n",
    "                        print(path_pdf)\n",
    "                        print (title)\n",
    "                        print (text)\n",
    "                        count += 1\n",
    "                        continue  # Retry with the next dataset element\n",
    "            ocr_df.to_csv(ocr_mapping_df_path, index=False)\n",
    "\n",
    "        elif (sub_class=='Font-poisoning_default'):\n",
    "            font_poisoning_mapping_df_path=\"../font_poisoning_mapping.csv\"\n",
    "            #if file does not exist, create new df\n",
    "            if not os.path.exists(font_poisoning_mapping_df_path):\n",
    "                font_poisoning_df=pd.DataFrame(columns=[\"file\", \"full_file_path\",\"title\",'text','joint_text'])\n",
    "                font_poisoning_df.to_csv(font_poisoning_mapping_df_path, index=False)\n",
    "            else:\n",
    "                font_poisoning_df=pd.read_csv(font_poisoning_mapping_df_path)\n",
    "    \n",
    "            for i in range(num_files):    \n",
    "                while True:  # Loop to retry if an error occurs\n",
    "                    try:\n",
    "                        title, text = extract_sanitized(dataset_iterator)\n",
    "\n",
    "\n",
    "                        joint_text = title + \"\\n\\n\" + text\n",
    "                        \n",
    "                        creation = super_class_map[super_class][sub_class]\n",
    "                        path_pdf_file=creation(path_pdf, title, text, i)\n",
    "                        print(path_pdf_file)\n",
    "\n",
    "                        file_name=os.path.basename(path_pdf_file)\n",
    "                        font_poisoning_df.loc[len(font_poisoning_df)] = {\n",
    "                            \"file\": file_name,\n",
    "                            \"full_file_path\": path_pdf_file,\n",
    "                            \"title\": title,\n",
    "                            \"text\": text,\n",
    "                            \"joint_text\": joint_text\n",
    "                        }\n",
    "\n",
    "                        break  # Exit the loop if successful\n",
    "                    except StopIteration:\n",
    "                        print(\"No more elements in the dataset.\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error occurred: {e}. Retrying with the next dataset element..., cout error {count}\")\n",
    "                        print(path_pdf)\n",
    "                        print (title)\n",
    "                        print (text)\n",
    "                        count += 1\n",
    "                        continue  # Retry with the next dataset element\n",
    "            font_poisoning_df.to_csv(font_poisoning_mapping_df_path,index=False)\n",
    "        \n",
    "        elif (sub_class ==  \"Diacritical-injection_mask1\" or sub_class ==  \"Diacritical-injection_mask2\") :#Camouflage Element\n",
    "            for i in range(num_files):    \n",
    "                \n",
    "                # Iterate until a title with a diacritical accent is found\n",
    "                found = False\n",
    "                while not found:\n",
    "                    try:\n",
    "                        title, text = extract_sanitized(dataset_iterator)\n",
    "\n",
    "                        \n",
    "                        if has_diacritical_accent(title):                       \n",
    "                            # Create the PDF\n",
    "                            print(path_pdf)\n",
    "                            creation = super_class_map[super_class][sub_class]\n",
    "                            print(creation(path_pdf, title, text, i))\n",
    "                            found = True\n",
    "                            \n",
    "                    except StopIteration:\n",
    "                        print(\"No more elements in the dataset.\")\n",
    "                        break  # Exit the loop if no more elements are available\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error occurred: {e}. Retrying with the next dataset element...\")\n",
    "                        continue  # Retry with the next element if an error occurs\n",
    "\n",
    "    \n",
    "        else :\n",
    "            for i in range(num_files):    \n",
    "                while True:  # Loop to retry if an error occurs\n",
    "                    try:\n",
    "                        title, text = extract_sanitized(dataset_iterator)\n",
    "                        \n",
    "                        creation = super_class_map[super_class][sub_class]\n",
    "                        print(creation(path_pdf, title, text, i))\n",
    "                        break  # Exit the loop if successful\n",
    "                    except StopIteration:\n",
    "                        print(\"No more elements in the dataset.\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error occurred: {e}. Retrying with the next dataset element..., cout error {count}\")\n",
    "                        print(path_pdf)\n",
    "                        print (title)\n",
    "                        print (text)\n",
    "                        count += 1\n",
    "                        continue  # Retry with the next dataset element\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
